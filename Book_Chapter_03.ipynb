{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 03: Algorithm Analysis\n",
    "\n",
    "* **Data structure** is a systematic way of organizing and accessing data.\n",
    "* **Algorithm** is a step-by-step procedure for performing some task in a finite amount of time.\n",
    "\n",
    "* Limitation of Experimental Analysis\n",
    "  1. Experimental running times is not an objective measure for comparing several algorithms since it is heavily rely on hardware and software environments.\n",
    "  2. Experiments can be done only on a limited set of test inputs. We don't know what will happen for cases not included in the experiment. \n",
    "  3. Experimental analysis requires actual implementation of code for measuring performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {}
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, our way of analyzing the efficiency of algorithms should overcome the defects above.\n",
    "\n",
    "1. Counting Primitive Operations\n",
    "  \n",
    "  We define a set of **primitive operations** such as following:\n",
    "  * Assigning an identifier to an object\n",
    "  * Determining the object associated with an identifier\n",
    "  * Performing an arithmetic operation\n",
    "  * Comparing two numbers\n",
    "  * Accessing a single element of a Python `list` by index\n",
    "  * Calling a function (excluding operations executed wihtin the function)\n",
    "  * Returning from a function\n",
    "  \n",
    "  Formally, a primitive operation corresponds to a low-level instruction with an execution time that is constant. Instead of trying to determine the specific execution time of each primitive operation, we will simply count how many primitive operations are executed, and use this number, $t$, of primitive operations an algorithm performs will be proportional to the actual running time of that algorihtm.\n",
    "  \n",
    "2. Measuring Operations as a Function of Input Size\n",
    "\n",
    "  To capture the order of growth of an algorithm's running time, we will associate, with each algorithm, a function $f(n)$ that characterizes the number of primitive operations that are performed as a function of the input isze $n$.\n",
    "  \n",
    "3. Focusing on the Worst-Case Input\n",
    "\n",
    "  An algorithm may run faster on some inputs than it does on others of the same size. Thus, we may wish to express the running time of an algorithm as the function of the input size obtained by taking the average over all possible inputs of the same size. Unfortunately, such an **average-case** analysis is typically quite challenging. It requires us to define a probability distribution on the set of inputs, which is often a difficult task.\n",
    "  \n",
    "  An average-case analysis usually requires that we calculate expected running times based on a given input distribution, which usually involves sophisticated probability theory. Therefore, we will characterize runnning times in terms of the **worst case**, as a function of the input size, $n$, of the algorithm.\n",
    "  \n",
    "  Worst-case analysis si much easier than average-case analysis, as it requires only the ability to identify the wors-case input, which is often simple."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 The Seven Functions Used in This Book\n",
    "\n",
    "1. The Constant Function\n",
    "    \n",
    "    $$f(n) = c$$\n",
    "\n",
    "2. The Logarithm Function\n",
    "\n",
    "    $$f(n) = \\log_b n$$\n",
    "    \n",
    "3. The Linear Function\n",
    "\n",
    "    $$f(n) = n$$\n",
    "\n",
    "4. The N-Log-N Function\n",
    "\n",
    "    $$f(n) = n \\log n$$\n",
    "\n",
    "5. The Quadratic Function\n",
    "\n",
    "    $$f(n) = n^2$$\n",
    "\n",
    "6. The Cubic Function and Other Polynomials\n",
    "\n",
    "    $$f(n) = n^3$$\n",
    "    \n",
    "    $$f(n) = a_0 + a_1n + a_2n^2 + a_3n^3 + \\cdots + a_dn^d$$\n",
    "    \n",
    "7. The Exponential Function\n",
    "\n",
    "    $$f(n) = b^n$$\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Asymptotic Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 The \"Big-O\" Notation\n",
    "Let $f(n)$ and $g(n)$ be the functions mapping positive integers to positive real umbers. We say that $f(n)$ is $O(g(n))$ if there is a real constant $c > 0$ and an integer constant $n_0 \\geq 1$ such that\n",
    "\n",
    "$$f(n) \\leq cg(n), \\quad \\text{for} \\quad n \\geq n_0$$\n",
    "\n",
    "This definition is often referred to as the \"big-O\" notation, for it is sometimes pronounced as $``f(n) \\ is \\ \\boldsymbol{big-O} \\ of \\ g(n).\"$\n",
    "\n",
    "The big-O notation allows us to say that a function $f(n)$ is \"less than or equal to\" another function $g(n)$ up to \n",
    "a constant factor and in the **asymptotic** sense as $n$ grows toward infinity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 Comperative Analysis\n",
    "Supposee two algorithms solving the same problem are available: an algorithm $A$, which has a running time of $O(n)$, \n",
    "and an algorithm $B$, which has a running time of $O(n^2)$. Which algorithm is better? We know that $n$ is $O(n^2)$, which implies that algorithm $A$ is \n",
    "**asymptotically better** than algorithm $B$. \n",
    "\n",
    "#### Some Words of Caution\n",
    "A few words of caution about asymptotic anotation are in order at this point. First, note that the use of the big-O and raelated notations can e somewhat misleading should the constant factors they \"hide\" be very large.\n",
    "For example, while it is true that the function $10^{100}n$ is $O(n)$. if this is the running time of an algorithm being compared to one whose running time is $10n\\log n$, we should prefer the $O(n\\log n)$ time algorithm, even though the linear-time algorithm is asymptotically faster.\n",
    "\n",
    "### 3.3.3 Examples of Algorithm Analysis\n",
    "\n",
    "#### Prefix Averages\n",
    "Given a sequence $S$ consisting of $n$ numbers, we want to compute a sequence $A$ such that $A[j]$ is the average of elements $S[0], \\ldots, S[j]$, for $j=0,\\ldots,n-1$, that is,\n",
    "\n",
    "$$A[j] = \\frac{\\sum_{i=0}^j s[i]}{j + 1}$$\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "##### A Quadratic-Time Algorithm\n\n```python\ndef prefix_quadratic(S):\n\n    n = len(S)                      #  O(1)\n    A = [0] * n                     #  O(n)\n    for j in range(n):              #  O(n)\n        total = 0\n        for i in range(j + 1):      #  O(n^2)\n            total += S[i]           #  O(n^2)\n        A[j] = total / (j+1)        #  O(n)\n    return A\n```\n\nTherefore, the running time of `prefix_quadratic` is $O(n^2)$\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "\n```python\ndef prefix_linear(S):\n\n    n = len(S)                      #  O(1)\n    A = [0] * n                     #  O(n)\n    total = 0\n    for j in range(n):              #  O(n)\n        total += S[j]               #  O(n)\n        A[j] = total / (j+1)        #  O(n)\n    return A\n```\n\nTherefore, the running time of `prefix_linear` is $O(n)$.\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Three-Way Set Disjointness\nSuppose we are given three sequences of numbers, $A, B$ and $C$. We will assume that no individual sequence contains duplicate values, but that there may be some numbers that are in two or three of the sequences.\nThe **three-way set disjointness** problem is to determine if the intersection of the three sequences is empty, namely, that there is no element $x$ such that $x \\in A, x \\in B$ and $x \\in C$.\n\n```python\ndef disjoint1(A, B, D):\n    for a in A:\n        for b in B:\n            for c in C:\n                if a == b == c:\n                    return False\n    return True\n```\n\nIf each of the original sets has size $n$, then the worst-case running time of this function is $O(n^3)$.\n\n```python\ndef disjoint2(A, B, D):\n    for a in A:\n        for b in B:\n            if a== b:\n                for c in C:\n                    if a == c:\n                        return False\n    return True\n```\n\nIn the improved version, it is not simply that we save time if we get lucky. We claim that the *worst-case* running time for `disjoint2` is $O(n^2)$, since the innermost loop, over $C$, executes at most $n$ times.\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Element Uniqueness\nA problem that is closely related to the three-way set disjointness problem is the **element uniqueness problem**. In the former, we are given three collections and we presumed that there were no duplicates within a single collection. IN the element uniqueness problem, we are given a single sequence $S$ with $n$ elements and asked whether all elements of that collection are distinct from each other.\n\n```python\ndef unique1(S):\n    for j in range(len(S)):\n        for k in range(j+1, len(S)):\n            if S[j] == S[k]:\n                return False\n    return True\n```\n\nThe approach is $O(n^2)$.\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Using Sorting as a Problem-Solving Tool\n\n```python\ndef unique2(S):\n    temp = sorted(S)\n    for j in range(1, len(temp)):\n        if S[j-1] == S[j]:\n        return False\n    return True\n```\n\nIt guarantees a worst-case running time of $O(n \\log n)$.",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}